## 1. 实验目的
*   理解虚拟文件系统（VFS）的抽象层次与设计思想。
*   掌握 Simple File System (SFS) 的磁盘布局与 I/O 处理逻辑。
*   实现进程管理与文件系统的深度集成，支持从磁盘加载并执行 ELF 程序。

---

## 2. 练习 1：SFS 读写核心逻辑实现

### 2.1 实现内涵
在 `kern/fs/sfs/sfs_inode.c` 的 `sfs_io_nolock` 函数中，我们处理了文件系统最核心的矛盾：**磁盘的块对齐访问与用户程序的字节流访问之间的不匹配**。

**核心代码逻辑：**
```c
// 循环处理每一块磁盘数据
while (alen < len) {
    size_t nbox, blkoff = offset % SFS_BLKSIZE;
    uint32_t blkno;
    // 1. 寻找/分配磁盘块
    if ((ret = sfs_bmap_load_nolock(sfs, sfs_inode, offset / SFS_BLKSIZE, &blkno)) != 0) goto out;
    
    // 2. 计算本次 I/O 长度
    nbox = MIN(len - alen, SFS_BLKSIZE - blkoff);
    
    // 3. 执行 I/O：如果是整块且对齐，直接操作磁盘；否则使用缓冲区
    if (nbox == SFS_BLKSIZE && blkoff == 0) {
        ret = sfs_block_op(sfs, buf, blkno, 1, write);
    } else {
        ret = sfs_buf_op(sfs, buf, nbox, blkno, blkoff, write);
    }
    if (ret != 0) goto out;
    
    // 4. 更新偏移与剩余长度
    alen += nbox, buf += nbox, offset += nbox;
}
```

### 2.2 深度解析
该实现采用了“三段式”逻辑：处理起始不对齐部分、循环处理中间对齐块、处理末尾残余。通过 `sfs_bmap_load_nolock` 将逻辑偏移转换为物理块号，实现了文件的线性抽象。
这体现了操作系统中**缓存（Buffering）**的重要性。对于不对齐的访问，内核必须先将整块读入内存缓冲区，修改部分字节后再写回，这被称为“读-改-写”过程。

## 3. 练习 2：基于文件系统的执行程序机制

### 3.1 进程与文件系统的集成
为了让进程能够访问文件，我们在 `proc_struct` 中引入了 `files_struct`。
*   **`do_fork` 中的继承**：通过 `copy_files` 实现了 UNIX 风格的文件描述符继承，使得子进程默认拥有与父进程相同的标准输入输出。
*   **`do_exit` 中的回收**：调用 `put_files` 确保进程退出时，所有打开的文件引用计数正确递减，防止 inode 泄露。

### 3.2 `load_icode` 的重构
这是 Lab8 最显著的变化：程序不再从内存镜像加载，而是从磁盘文件加载。

**关键步骤：**
1.  **读取 ELF 头部**：通过文件描述符 `fd` 读取并校验 ELF 格式。
2.  **建立内存映射**：遍历程序头表（Program Header），为每个 `PT_LOAD` 段调用 `mm_map` 建立 VMA。
3.  **按需加载**：分配物理页并调用 `sfs_io` 将磁盘内容载入内存。
4.  **用户栈参数传递**：这是实现 `main(argc, argv)` 的关键。

**参数压栈逻辑：**
```c
// 在用户栈顶分配空间并拷贝参数字符串
uintptr_t stacktop = USTACKTOP - argc * sizeof(char *);
char **uargv = (char **)stacktop;
for (i = 0; i < argc; i++) {
    uargv[i] = str_ptr; // 指向已拷贝到栈上的字符串
}
// 压入 argc
stacktop -= sizeof(int);
*(int *)stacktop = argc;
```
## 4. 知识点总结与对比

### 4.1 重要知识点
1.  **VFS 抽象层**：通过 `inode_ops` 和 `fs_ops` 屏蔽了底层文件系统的差异。这对应了 OS 原理中的**多态性**和**设备独立性**。
2.  **索引节点（Inode）**：SFS 使用索引结构管理文件块。对应原理中的**混合索引分配**。
3.  **文件描述符表**：进程通过整数索引访问文件。对应原理中的**句柄（Handle）**机制。

### 4.2 原理与实验的差异
*   **差异**：原理中常讨论复杂的目录项缓存（dentry cache）和日志文件系统（Journaling），而 ucore 的 SFS 相对简化，没有实现复杂的崩溃恢复机制。
*   **未涉及点**：实验中未涉及**网络文件系统（NFS）**、**磁盘调度算法（如 SCAN）**以及**文件权限与访问控制列表（ACL）**。

## 5. 扩展挑战：Challenge 设计方案(具体设计见./challenge.md)

### 5.1 Challenge 1: UNIX PIPE 机制
*   **设计核心**：将管道抽象为一种**内存文件**。
*   **数据结构**：定义 `struct pipe_buffer`，包含循环缓冲区和两个信号量（`sem_read`, `sem_write`）。
*   **同步处理**：利用生产-消费模型。当缓冲区满时，写进程在 `sem_write` 上阻塞；当缓冲区空时，读进程在 `sem_read` 上阻塞。

### 5.2 Challenge 2: 硬链接与软链接
*   **硬链接**：在目录项中增加指向同一 Inode 的记录，并增加 Inode 的 `nlinks` 计数。只有当 `nlinks` 为 0 时才释放磁盘空间。
*   **软链接**：创建一种特殊类型的文件（`SFS_TYPE_LINK`），其数据块存储目标文件的路径字符串。在 `vfs_lookup` 时，若遇到此类文件，则递归解析其存储的路径。

---

## 6. 实验结论
通过本次实验，我们成功构建了一个支持磁盘文件系统、多进程并发访问文件、以及能够加载执行磁盘程序的完整内核。运行 `make qemu` 后，`sh` 的成功启动证明了 VFS、SFS 与进程管理模块之间协作的正确性。